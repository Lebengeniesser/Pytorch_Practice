{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4W4/azHGo4GqKCuEj48Vg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lebengeniesser/Pytorch_Practice/blob/master/%EC%98%88%EC%A0%9C%EB%A1%9C_%EB%B0%B0%EC%9A%B0%EB%8A%94_%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm1Jh04mEKB0",
        "outputId": "264bea0f-0932-4a50-c068-3208a77e6c58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 3290.1234906665286\n",
            "199 2330.841853639788\n",
            "299 1652.0071822688828\n",
            "399 1171.6272102047465\n",
            "499 831.6844419257623\n",
            "599 591.122591506794\n",
            "699 420.88802963668485\n",
            "799 300.4208481764237\n",
            "899 215.1717506942424\n",
            "999 154.84487180231974\n",
            "1099 112.15430225375201\n",
            "1199 81.94413956559254\n",
            "1299 60.56578868203662\n",
            "1399 45.4373065037672\n",
            "1499 34.73157039503099\n",
            "1599 27.155609439406796\n",
            "1699 21.79444734800987\n",
            "1799 18.000597279184376\n",
            "1899 15.315862406799988\n",
            "1999 13.415997776366625\n",
            "Result: y = 0.07178095819699351+0.8565921549070911x+-0.012383409729281494x^2+-0.09330923944367465x^3\n",
            "99 2582.837890625\n",
            "199 1815.859130859375\n",
            "299 1278.0068359375\n",
            "399 900.6360473632812\n",
            "499 635.7311401367188\n",
            "599 449.6878662109375\n",
            "699 318.9712829589844\n",
            "799 227.0890655517578\n",
            "899 162.47799682617188\n",
            "999 117.02677917480469\n",
            "1099 85.04228210449219\n",
            "1199 62.526878356933594\n",
            "1299 46.67210006713867\n",
            "1399 35.50421142578125\n",
            "1499 27.635480880737305\n",
            "1599 22.089792251586914\n",
            "1699 18.180349349975586\n",
            "1799 15.423724174499512\n",
            "1899 13.479543685913086\n",
            "1999 12.108060836791992\n",
            "Result: y = 0.05959932506084442 + 0.8674127459526062 x + -0.010281872935593128 x^2 + -0.09484837204217911 x^3\n",
            "tensor([1, 2, 3])\n",
            "tensor([[ -3.1416,   9.8696, -31.0063],\n",
            "        [ -3.1384,   9.8499, -30.9133],\n",
            "        [ -3.1353,   9.8301, -30.8205],\n",
            "        ...,\n",
            "        [  3.1353,   9.8301,  30.8205],\n",
            "        [  3.1384,   9.8499,  30.9133],\n",
            "        [  3.1416,   9.8696,  31.0063]])\n",
            "99 233.1266632080078\n",
            "199 162.44400024414062\n",
            "299 114.15536499023438\n",
            "399 81.12753295898438\n",
            "499 58.51151657104492\n",
            "599 43.00733184814453\n",
            "699 32.36647033691406\n",
            "799 25.055086135864258\n",
            "899 20.02574920654297\n",
            "999 16.562332153320312\n",
            "1099 14.174607276916504\n",
            "1199 12.526763916015625\n",
            "1299 11.388311386108398\n",
            "1399 10.600964546203613\n",
            "1499 10.055878639221191\n",
            "1599 9.678146362304688\n",
            "1699 9.416131973266602\n",
            "1799 9.234204292297363\n",
            "1899 9.107776641845703\n",
            "1999 9.019834518432617\n",
            "Result: y = 0.013395425863564014 + 0.8504027128219604x + -0.002310933545231819 x^2 + -0.09242884814739227 x^3\n"
          ]
        }
      ],
      "source": [
        "#<PyTorch: NumPy>\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "#무작위로 입력과 출력 데이터를 생성\n",
        "x = np.linspace(-math.pi, math.pi, 2000)\n",
        "y = np.sin(x)\n",
        "#무작위로 가중치를 초기화 함\n",
        "a = np.random.randn()\n",
        "b = np.random.randn()\n",
        "c = np.random.randn()\n",
        "d = np.random.randn()\n",
        "\n",
        "learning_rate = 1e-6 # 10^-6\n",
        "for t in range(2000):\n",
        "  #순전파 단계: 예측값 y를 계산\n",
        "  #y = a + bx +c x^2 + d x^3\n",
        "  y_pred = a + b *x + c *x**2 + d*x**3\n",
        "  #손실(loss)을 계산하고 출력함\n",
        "  loss = np.square(y_pred - y).sum()\n",
        "  if t % 100 == 99:\n",
        "    print(t, loss)\n",
        "\n",
        "  #손실에 따른 a,b,c,d의 변화도를 계산하고 역전파 한다.\n",
        "  grad_y_pred = 2.0 * (y_pred - y)  \n",
        "  grad_a = grad_y_pred.sum()\n",
        "  grad_b = (grad_y_pred * x).sum()\n",
        "  grad_c = (grad_y_pred * x**2).sum()\n",
        "  grad_d = (grad_y_pred * x**3).sum()\n",
        "\n",
        "  a -= learning_rate *grad_a\n",
        "  b -= learning_rate *grad_b\n",
        "  c -= learning_rate *grad_c\n",
        "  d -= learning_rate *grad_d\n",
        "\n",
        "print(f\"Result: y = {a}+{b}x+{c}x^2+{d}x^3\")\n",
        "\n",
        "#<PyTorch: Tensor>\n",
        "import torch\n",
        "import math\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "#무작위로 입력과 출력 데이터를 생성\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device = device, dtype = dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "#무작위로 가주잋를 초기화\n",
        "a = torch.randn((), device = device, dtype = dtype)\n",
        "b = torch.randn((), device = device, dtype = dtype)\n",
        "c = torch.randn((), device = device, dtype = dtype)\n",
        "d = torch.randn((), device = device, dtype = dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "  y_pred = a + b*x + c*x**2 + d*x**3\n",
        "  loss = (y_pred - y).pow(2).sum().item()\n",
        "  if t % 100 == 99:\n",
        "    print(t, loss)\n",
        "  grad_y_pred = 2.0 * (y_pred - y)  \n",
        "  grad_a = grad_y_pred.sum()\n",
        "  grad_b = (grad_y_pred * x).sum()\n",
        "  grad_c = (grad_y_pred * x**2).sum()\n",
        "  grad_d = (grad_y_pred * x**3).sum()\n",
        "\n",
        "  a -= learning_rate *grad_a\n",
        "  b -= learning_rate *grad_b\n",
        "  c -= learning_rate *grad_c\n",
        "  d -= learning_rate *grad_d\n",
        "\n",
        "print(f\"Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3\")\n",
        "#NumPy 와 Tensor의 연산을 비교하였을 때 Tensor가 손실이 더 적었다.\n",
        "\n",
        "#Autograd 신경망의 순전파 단계에서 연산그래프를 정의하게 됨.\n",
        "#이 그래프의 노드는 텐서이고, 엣지는 입력텐서로 부터 출력텐서르 만들어내는 함수(노드=텐서, 엣지=출력)\n",
        "#  y = a + bx + cx^2+dx^3 대신 y = a + bP3(c+dx)모델을 정의함.\n",
        "#P3(x) = 1/2 (5x^3 -3x)은 3차 르장드르다항식이다. P3의 순전파와 역전파 연산을 위한 새로운 autograd Function를 작성하고, 이를 사용하여 모델을 구현\n",
        "\n",
        "#PyTorch: nn (인공신경망)\n",
        "#Tensorflow는 Keras, TensorFlow-Slim, TFLearn같은 패키지들이 연산 그래프를 고수준으로 추상화 하여 제공하므로 신경망을 구축하는데 유용함\n",
        "\n",
        "import torch\n",
        "import math\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "#이예제는 선형함수이므로, 선형 계층 신경망으로 간주\n",
        "p = torch.tensor([1,2,3])\n",
        "xx = x.unsqueeze(-1).pow(p) # pow(y,x) y^x라는 뜻 허수면 에러\n",
        "print(p)\n",
        "print(xx)\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3,1),\n",
        "    torch.nn.Flatten(0,1)\n",
        ")\n",
        "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "  y_pred = model(xx)\n",
        "  loss = loss_fn(y_pred, y)\n",
        "  if t % 100 == 99:\n",
        "    print(t, loss.item())\n",
        "\n",
        "  #역전파 단계를 실행하기 전에 변화도를 0으로 만든다.\n",
        "  model.zero_grad()  \n",
        "\n",
        "  #역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해 손실의 변화도를 계산한다.\n",
        "  #내보적으로 각 Module의 매개변수는 requires_grad=True일 때 텐서에 저장되므로,\n",
        "  # 아래 호출은 모델의 모든 학습 가능한 매개변수의 변화도를 계산하게 된다.\n",
        "  loss.backward()\n",
        "\n",
        "  #경사하강법을 사용하여 가중치를 갱신한다.\n",
        "  #각 매개변수는 텐서이므로, 이전에 했던 것처럼 변화도에 접근할 수 있다.\n",
        "  with torch.no_grad():\n",
        "    for param in model.parameters():\n",
        "      param -= learning_rate * param.grad\n",
        "linear_layer = model[0]      \n",
        "\n",
        "#선형 계층에서, 매개변수는 'weights'와 'bias'로 저장된다.\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()}x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n",
        "\n",
        "#NumPy, Tensor, Autograd확인 결과 Autograd의 손실이 더 적었다.\n",
        "\n",
        "#PyTorch: optim패키지는 최적화 알고리즘에 대한 아이디어를 추상화하고 일반적으로 사용하는 최적화 알고리즘의 구현체를 제공한다.\n",
        "#지금까지 nn패키지를 사용하여 모델을 저으이하지만, 모델을 최적화할 때는 optim패키지가 제공하는 RMSProp알고리즘을 사용한다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "92gv69cQMWO5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}